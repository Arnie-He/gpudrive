train:
  seed: 1
  torch_deterministic: True
  device: cpu
  cpu_offload: True
  total_timesteps: 100_000_000
  learning_rate: 2.5e-3
  num_steps: 90
  anneal_lr: True
  gamma: 0.99
  gae_lambda: 0.95
  num_minibatches: 4
  update_epochs: 24
  norm_adv: True
  clip_coef: 0.1
  clip_vloss: false
  ent_coef: 0.01
  vf_coef: 1.0
  max_grad_norm: 0.5
  target_kl: ~

  num_envs: 8
  
  envs_per_worker: 1
  envs_per_batch: ~
  env_pool: True
  verbose: True
  data_dir: experiments
  checkpoint_interval: 200
  pool_kernel: [0]
  batch_size: 1024
  minibatch_size: 512
  bptt_horizon: 1 #8
  vf_clip_coef: 1.0
  compile: False
  compile_mode: reduce-overhead

env_params:
  action_space_type: continuous

reward_params:
  rewardType: OnGoalAchieved  
  distanceToGoalThreshold: 3.0
  distanceToExpertThreshold: 50.0

parameters:
  polylineReductionThreshold: 0.5
  observationRadius: 150.0
  datasetInitOptions: FirstN
  rewardParams: reward_params
  collisionBehaviour: AgentStop
  maxNumControlledVehicles: 1
  IgnoreNonVehicles: True
  roadObservationAlgorithm: AllEntitiesWithRadiusFiltering
  initOnlyValidAgentsAtFirstStep: True
  useWayMaxModel: False
  enableLidar: False
  disableClassicalObs: False
  isStaticAgentControlled: False

sim_manager:
  exec_mode: CPU
  gpu_id: 0
  num_worlds: 3
  json_path: /Users/aaravpandya/dev/gpudrive/example_data
  enable_batch_renderer: False