res_path: examples/experiments/eval/dataframes # Store dataframes here
test_dataset_size: 100 # Number of test scenarios to evaluate on

# Environment settings
data_dir: "data/processed/training"
num_worlds: 10 # Number of parallel environments for evaluation
max_controlled_agents: 128 # Maximum number of agents controlled by the model.
ego_state: true
road_map_obs: true
partner_obs: true
normalize_obs: true
remove_non_vehicles: true # If false, all agents are included (vehicles, pedestrians, cyclists)
use_lidar_obs: false # NOTE: Setting this to true currently turns of the other observation types
reward_type: "weighted_combination"
collision_weight: -0.1
off_road_weight: -0.1
goal_achieved_weight: 1.0
dynamics_model: "classic"
collision_behavior: "ignore" # Options: "remove", "stop"
dist_to_goal_threshold: 4.0
polyline_reduction_threshold: 0.1 # Rate at which to sample points from the polyline (0 is use all closest points, 1 maximum sparsity), needs to be balanced with kMaxAgentMapObservationsCount
sampling_seed: 42 # If given, the set of scenes to sample from will be deterministic, if None, the set of scenes will be random
obs_radius: 50.0 # Visibility radius of the agents

device: "cuda" # Options: "cpu", "cuda"


model_path: examples/experiments/eval/models/model_PPO__S_100__01_02_12_56_49_981_000800_cb_rmv.pt #examples/experiments/eval/models/model_PPO__R_1000__01_01_16_47_36_976_000600.pt
