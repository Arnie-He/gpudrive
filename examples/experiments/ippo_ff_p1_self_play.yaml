mode: "train" # Options: "train", "sweep"
use_rnn: false
eval_model_path: null
baseline: false
data_dir: "data/processed/examples"

environment: # Overrides default environment configs (see pygpudrive/env/config.py)
  name: "gpudrive"
  num_worlds: 100 # Number of parallel environments
  k_unique_scenes: 3 # Number of unique scenes to sample from
  max_controlled_agents: 64 # Maximum number of agents controlled by the model. Make sure this aligns with the variable kMaxAgentCount in src/consts.hpp
  ego_state: true
  road_map_obs: true
  partner_obs: true
  norm_obs: true
  remove_non_vehicles: true # If false, all agents are included (vehicles, pedestrians, cyclists)
  lidar_obs: false # NOTE: Setting this to true currently turns of the other observation types
  reward_type: "weighted_combination" #"distance_to_logs"
  dynamics_model: "classic"
  collision_behavior: "ignore" # Options: "remove", "stop", "ignore"
  dist_to_goal_threshold: 2.0
  polyline_reduction_threshold: 0.1 # Rate at which to sample points from the polyline (0 is use all closest points, 1 maximum sparsity), needs to be balanced with kMaxAgentMapObservationsCount
  sampling_seed: 42 # If given, the set of scenes to sample from will be deterministic, if None, the set of scenes will be random
  obs_radius: 50.0 # Visibility radius of the agents
wandb:
  entity: "emerge_"
  project: "self_play_rl"
  group: "verify"
  mode: "online" # Options: online, offline, disabled
  tags: ["ppo", "ff", "basic-sweep"]

## NOTES
## Good batch size: 128 * number of controlled agents (e.g. 2**18)
## Minibatch size 1/16 of batch size, eg. 16_000

train:
  exp_id: PPO # Set dynamically in the script if needed
  seed: 42
  cpu_offload: false
  device: "cuda"  # Dynamically set to cuda if available, else cpu
  bptt_horizon: 1
  compile: false
  compile_mode: "reduce-overhead"

  # # # Data sampling # # #
  resample_scenes: False
  resample_criterion: "global_step"
  resample_dataset_size: 500 # Number of unique scenes to sample from
  resample_interval: 20_000_000
  resample_limit: 1000 # Resample until the limit is reached; set to a large number to continue resampling indefinitely
  sample_with_replacement: true
  shuffle_dataset: false

  # # # PPO # # #
  torch_deterministic: false
  total_timesteps: 100_000_000
  batch_size: 131_072
  minibatch_size: 16_384
  learning_rate: 3e-4
  anneal_lr: false
  gamma: 0.99
  gae_lambda: 0.95
  update_epochs: 1
  norm_adv: true
  clip_coef: 0.2
  clip_vloss: false
  vf_clip_coef: 0.2
  ent_coef: 0.001
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: null
  collision_weight: 0.05 # Note: Assuming we want to avoid collisions and off-road events
  off_road_weight: 0.05
  goal_achieved_weight: 1.0
  # Settings below introduce a warmup period where the collision penalties are
  # set to zero and then gradually increase. Control the fraction of the warmup period during which penalties are zero.
  # For example, config.penalty_start_frac = 0.3 means the first 30% of the warmup steps has no penalties.
  collision_penalty_warmup: false
  penalty_start_frac: 0.2
  warmup_steps: 100_000_000 # Should be somewhat proportional to the total number of training

  # # # Network # # #
  network:
    input_dim: 64 # Embedding of the input features
    hidden_dim: 192 # Latent dimension
    pred_heads_arch: [64] # Arch of the prediction heads (actor and critic)
    num_transformer_layers: 0 # Number of transformer layers
    dropout: 0.01
    class_name: "LateFusionTransformer"
    num_parameters: 0 # Total trainable parameters, to be filled at runtime

  # # # Checkpointing # # #
  checkpoint_interval: 500 # Save policy every k iterations
  checkpoint_path: "./runs"

  # # # Rendering # # #
  render: false # Determines whether to render the environment (note: will slow down training)
  render_interval: 500 # Render every k iterations
  render_k_scenarios: 10 # Number of scenarios to render
  render_simulator_state: true # Plot the simulator state from bird's eye view
  render_agent_obs: false # Debugging tool, plot what an agent is seing
  render_format: "mp4" # Options: gif, mp4
  render_fps: 15 # Frames per second

sweep:
  train:
    learning_rate:
      distribution: "log_normal"
      mean: 0.005
      scale: 1.0
      clip: 2.0

    ent_coef:
      distribution: "log_normal"
      mean: 0.005
      scale: 0.5
      clip: 1.0

    gamma:
      distribution: "logit_normal"
      mean: 0.98
      scale: 0.5
      clip: 1.0

    gae_lambda:
      distribution: "logit_normal"
      mean: 0.95
      scale: 0.5
      clip: 1.0

    collision_weight:
      distribution: "uniform"
      min: 0.05
      max: 1.0

    off_road_weight:
      distribution: "uniform"
      min: 0.05
      max: 1.0

    goal_achieved_weight:
      distribution: "uniform"
      min: 0.0
      max: 1.0

vec:
  backend: "native" # Only native is currently supported
  num_workers: 1
  env_batch_size: 1
  zero_copy: false
