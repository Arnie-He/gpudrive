# GAIL Configuration - Optimized for Fast Training with State-Only Discriminator
# The discriminator only takes states as input, not state-action pairs
# Smaller batch sizes for reasonable inner loop times

mode: "train"
use_rnn: false
eval_model_path: null
baseline: false
data_dir: data/processed/training
continue_training: false
model_cpt: null

expertdata:
  remake: false

environment:
  name: "gpudrive"
  num_worlds: 75  # Reduced for faster training
  k_unique_scenes: 75
  max_controlled_agents: 64 # Maximum number of agents controlled by the model. Make sure this aligns with the variable kMaxAgentCount in src/consts.hpp
  ego_state: true
  road_map_obs: true
  partner_obs: true
  norm_obs: true
  remove_non_vehicles: true
  lidar_obs: false
  reward_type: "weighted_combination"
  collision_weight: -0.75
  off_road_weight: -0.75
  goal_achieved_weight: 1.0
  dynamics_model: "classic"
  collision_behavior: "ignore"
  dist_to_goal_threshold: 2.0
  polyline_reduction_threshold: 0.1
  sampling_seed: 42
  obs_radius: 50.0
  action_type: "discrete"  # Action space type: "discrete" or "continuous"
  action_space_steer_disc: 13
  action_space_accel_disc: 7
  use_vbd: false
  vbd_model_path: "gpudrive/integrations/vbd/weights/epoch=18.ckpt"
  init_steps: 11
  vbd_trajectory_weight: 0.1
  vbd_in_obs: false

wandb:
  entity: "rl-power"
  project: "gpudrive"
  group: "fast-gail"
  mode: "online"
  tags: ["gail", "fast"]

train:
  # General training parameters
  exp_id: "gail_fast"
  seed: 42
  cpu_offload: false
  device: "cuda"
  bptt_horizon: 1
  compile: false
  compile_mode: "reduce-overhead"
  use_gradient_penalty: true
  gradient_penalty_lambda: 10.0
  grad_clip_norm: 1.0
  disc_updates_per_policy_update: 1
  visualize: false
  vis_interval: 1000
  vis_rollout_length: 100
  vis_num_rollouts: 10

  # Data sampling
  resample_scenes: false
  resample_dataset_size: 10_000
  resample_interval: 2_000_000
  sample_with_replacement: true
  shuffle_dataset: false

  # # # PPO # # #
  torch_deterministic: false
  total_timesteps: 20_000_000
  batch_size: 2048
  minibatch_size: 128
  learning_rate: 3e-4
  anneal_lr: false
  gamma: 0.99
  gae_lambda: 0.95
  update_epochs: 4
  norm_adv: true
  clip_coef: 0.2
  clip_vloss: false
  vf_clip_coef: 0.2
  ent_coef: 0.0001
  vf_coef: 0.3
  max_grad_norm: 0.5
  target_kl: null
  log_window: 1000
  
  # L2 Regularization
  policy_weight_decay: 0.0001        # L2 regularization for policy network
  value_weight_decay: 0.0001         # L2 regularization for value network
  
  # GAIL - OPTIMIZED TO PREVENT DISCRIMINATOR OVERFITTING
  use_gail: true
  discriminator_lr: 0.0003             # REDUCED from 0.001 to slow down discriminator learning
  discriminator_hidden_dim: 64         # REDUCED from 128 to weaken discriminator
  discriminator_dropout: 0.3           # INCREASED from 0.1 to 0.3 for more regularization
  discriminator_batch_size: 256        # REDUCED from 512 to make training less stable
  discriminator_epochs: 10              # REDUCED from 2 to 1 to prevent overfitting
  discriminator_update_freq: 30         # INCREASED from 1 to 5 - update discriminator less frequently
  policy_buffer_size: 20000            # Smaller buffer for memory efficiency
  min_policy_data: 1024                # INCREASED from 512 to accumulate more data before training
  discriminator_weight_decay: 0.0001    # INCREASED from 0.0001 for more regularization
  
  # Network architecture
  network:
    input_dim: 64
    hidden_dim: 128
    dropout: 0.01
    class_name: "NeuralNet"
    num_parameters: 0
    using_shared_embedding: true
  
  # Checkpointing
  checkpoint_interval: 400
  checkpoint_path: "./runs"

  # Rendering
  render: false # Determines whether to render the environment (note: will slow down training)
  render_3d: false # Render simulator state in 3d or 2d
  render_interval: 50 # Render every k iterations
  render_k_scenarios: 3 # Number of scenarios to render
  render_format: "mp4" # Options: gif, mp4
  render_fps: 15 # Frames per second
  zoom_radius: 50

vec:
  num_envs: 1
  async_: true 