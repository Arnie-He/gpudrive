# Single-Agent GAIL Configuration
# Optimized for training individual agents across many single-agent environments

# Data directory
data_dir: "/data/formatted_json_files_v2"

# Wandb logging configuration
wandb:
  project: "gpudrive"
  entity: null
  group: "single_agent_gail"
  mode: "online"
  tags: ["single_agent", "gail", "irl"]

# Environment configuration
environment:
  name: "gpudrive"
  num_worlds: 64  # Reduced for faster training
  k_unique_scenes: 64
  max_controlled_agents: 1
  ego_state: true
  road_map_obs: true
  partner_obs: true
  norm_obs: true
  remove_non_vehicles: true
  lidar_obs: false
  reward_type: "weighted_combination"
  collision_weight: -0.75
  off_road_weight: -0.75
  goal_achieved_weight: 1.0
  dynamics_model: "classic"
  collision_behavior: "ignore"
  dist_to_goal_threshold: 2.0
  polyline_reduction_threshold: 0.1
  sampling_seed: 42
  obs_radius: 50.0
  action_space_steer_disc: 13
  action_space_accel_disc: 7
  use_vbd: false
  vbd_model_path: "gpudrive/integrations/vbd/weights/epoch=18.ckpt"
  init_steps: 11
  vbd_trajectory_weight: 0.1
  vbd_in_obs: false

# Training configuration
train:
  # Experiment configuration
  exp_id: "single_agent_gail_experiment"
  seed: 42
  device: "cuda"  # Will auto-detect if available
  render: false
  continue_training: false
  model_cpt: null
  
  # Training hyperparameters
  total_timesteps: 5000000  # 5M timesteps
  learning_rate: 3e-4
  anneal_lr: false
  
  # PPO parameters (optimized for single-agent environments)
  batch_size: 8192   # Smaller than multi-agent since we have many single-agent envs
  minibatch_size: 1024
  update_epochs: 4
  gamma: 0.995
  gae_lambda: 0.95
  ent_coef: 0.01
  vf_coef: 0.5
  clip_coef: 0.2
  clip_vloss: true
  max_grad_norm: 0.5
  
  # GAIL-specific parameters
  discriminator_lr: 3e-4
  discriminator_hidden_dim: 256
  discriminator_dropout: 0.1
  discriminator_batch_size: 512
  discriminator_epochs: 3
  discriminator_update_freq: 1
  policy_buffer_size: 20000  # Larger buffer for diverse single-agent data
  min_policy_data: 2000
  
  # L2 Regularization
  policy_weight_decay: 1e-4
  value_weight_decay: 1e-4
  discriminator_weight_decay: 1e-4
  
  # Single-agent specific settings
  max_single_agent_envs: 64  # Limit number of single-agent environments (null for all)
  
  # Scene resampling
  resample_scenes: false
  resample_interval: 500000
  resample_dataset_size: 500
  sample_with_replacement: true
  shuffle_dataset: false
  
  # Network architecture
  network:
    input_dim: 268
    hidden_dim: 512
    dropout: 0.1

# Vector environment configuration
vec:
  num_workers: 1 